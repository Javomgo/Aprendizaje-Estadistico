---
title: "Ortiz_Javier_GooglePSApps"
author: "Javier Ortiz Montenegro"
date: "8 de enero de 2019"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Parte del código está extraida de: "https://www.kaggle.com/q2000qfq/rate-predictor", ejemplo drug001 y ejemplo XGBoost.

Los datos son distintas caracteristicas de un conjunto de apps de Google Play Store entre las que podemos encontrar la cantidad de descargas, el rating, la cantidad de reviews, a que categoría pertenecen, precio, edad objetivo etc.
He considerado que era una base de datos interesante ya que la cantidad de apps que se lanzan al año es enorme y creo que podría ser interesante realizar una predicción de las descargas que va a tener una app antes de ser lanzada en funcion de a que categoría pertenece, la edad del publico objetivo, su precio, etc.
Esto ayudaría a las empresas desarrolladoras de apps a una asignación eficiente de sus activos a los proyectos con mayor previsión.

Una vez realizada la introducción del dataset se procederá al tratamiento.

Se empieza cargando las librerías necesarias.

```{r message = FALSE, warning = FALSE}
library(dplyr)
library(tidyr)
library(xgboost)
require("caret")
require("randomForest")
require("gbm")
```

Posteriormente se realiza la carga y tratamiento de los datos. Al realizar un "summary" de los datos cargados nos encontramos frente a varios problemas, algunos errores en valores imposibles (Rating de 19 cuando unicamente es de 1 a 5), una observación se ha introducido incorrectamente, NAs, practicamente todas las variables como factor, etc...

Especial mención al tratamiento de la variable "Installs" variable que se tratará de predecir. En estos datos aparece como una variable discreta. Por lo tanto, se ha optado por unificar la cantidad de "Installs" en cuatro categorías (Menos de 50K, entre 50K y 1M, entre 1M y 10M y más de 10M), de esta manera se convierte en un problema de clasificación y se tratará como tal.
No obstante, comentar que de forma autonoma he realizado el mismo estudio convirtiendo los datos a numerico y realizando una predicción. Pero debido a unos mejores resultados mediante clasificación he decidido decantarme finalmente por ese método.

```{r}
#Carga de datos y revisión de los mismos.
apps.df = read.csv("googleplaystore.csv")
summary(apps.df)
str(apps.df)

#Tratamiento de valores atipicos, NA y duplicados.
apps.df$Installs[apps.df$Installs == "Free" ] = NA
apps.df$Rating[apps.df$Rating == 19] = NA
apps.df$Size[apps.df$Size == "Varies with device"] = NA
apps.df$Content.Rating[apps.df$Content.Rating == "Unrated" | apps.df$Content.Rating == "Adults only 18+"] = NA
apps.df = mutate(apps.df[,-c(10:13)]) %>% na.omit %>% droplevels() #En esta línea elimino las ultimas columnas del dataset ya que corresponden a la versión de la app, de android y la ultima vez actualizados, datos que me parecen poco importantes para la clasificación de la cantidad de descargas.
apps.df = apps.df[!duplicated(apps.df),]

#Tratamiento de Reviews pasando de factor a numérico.
apps.df$Reviews = as.numeric(as.character(apps.df$Reviews))

#Tratamiento de Size eliminando las letras del factor para poder transformar a numérico.
apps.df$Size = as.character(apps.df$Size)
size_express_mb <- "[0-9]+[[:punct:]]?[0-9]*M"
size_express_kb <- "[0-9]+[[:punct:]]?[0-9]*k"

index_mb <- grep(apps.df$Size,pattern = size_express_mb)
mb.match = regexpr(apps.df$Size, pattern = size_express_mb)
mb = regmatches(apps.df$Size,mb.match)

mb1 <- substr(mb, 1,nchar(mb) - 1)
mb1 <- as.numeric(mb1)*1000
apps.df$Size[index_mb] <- mb1

index_kb <- grep(apps.df$Size,pattern = size_express_kb)
kb.match = regexpr(apps.df$Size, pattern = size_express_kb)
kb = regmatches(apps.df$Size,kb.match)

kb1 <- substr(kb, 1,nchar(kb) - 1)
kb1 <- as.numeric(kb1)
apps.df$Size[index_kb] <- kb1
apps.df$Size = as.numeric(apps.df$Size)

#Tratamiento de Installs, unificando en 4 factores.
apps.df$Installs <- as.character(apps.df$Installs)

index1 = which(apps.df$Installs == "1+" | apps.df$Installs == "5+" |
                apps.df$Installs == "10+" | apps.df$Installs == "50+" |
                apps.df$Installs == "100+" | apps.df$Installs == "500+" |
                 apps.df$Installs == "1,000+" | apps.df$Installs == "5,000+" |
               apps.df$Installs == "10,000+")

index2 = which(apps.df$Installs == "50,000+" | apps.df$Installs == "100,000+" | apps.df$Installs == "500,000+")

index3 = which(apps.df$Installs == "1,000,000+" | apps.df$Installs == "5,000,000+")

index4 = which(apps.df$Installs == "10,000,000+" | apps.df$Installs == "50,000,000+" |
                 apps.df$Installs == "100,000,000+" | apps.df$Installs == "500,000,000+" |
                 apps.df$Installs == "1,000,000,000+")

apps.df$Installs[index1] <- "Under 50k+"
apps.df$Installs[index2] <- "50k+ to 500k+"
apps.df$Installs[index3] <- "1m+ to 5m+"
apps.df$Installs[index4] <- "Over 10m"

level <- c("Under 50k+", "50k+ to 500k+", "1m+ to 5m+", "Over 10m")
apps.df$Installs <- factor(apps.df$Installs, levels = level)

#Tratamiento de la variable Price eliminando el símbolo "$".

apps.df$Price = as.character(apps.df$Price)
apps.df$Price = substr(apps.df$Price, 2, nchar(apps.df$Price))
apps.df$Price[apps.df$Price == ""] = 0
apps.df$Price = as.numeric(apps.df$Price)

#Comprobación de los datos.
summary(apps.df)
str(apps.df)
```

Se separan los datos en train/validation/test una vez tratados.

```{r}
#Definimos una semilla para que el ejercicio sea reproducible
set.seed(2019)
# Se separa de la siguiente manera: 60% a train, 20% a validate y 20% a test
inTraining <- createDataPartition(apps.df$Installs, p=0.6, list=FALSE)
training.set <- apps.df[inTraining,]
Totalvalidation.set <- apps.df[-inTraining,]
# Ahora creamos una nueva partición del 40% de los datos, 20% a testing y 20% a validation
inValidation <- createDataPartition(Totalvalidation.set$Installs, p=0.5, list=FALSE)
testing.set <- Totalvalidation.set[inValidation,]
validation.set <- Totalvalidation.set[-inValidation,]

#Para el tuning y la comparación de modelos se usaran los datos de "dataset" y "validation".
dataset <- training.set
validation <- validation.set
test <- testing.set

# El modelo final usara todos los datos. "Dataset" y "validation" agrupados en "total" y "test".
total <- rbind(dataset, validation)
```

Se pasa a definir los parametros de control y el grid para realizar el tuning de los distintos modelos a comparar.

```{r}
set.seed(2019)
fitControl <- trainControl(method = 'cv', number = 5, summaryFunction=defaultSummary)


#El primer Grid es el de boosting.
gbmGrid <-  expand.grid(interaction.depth = c(1,4,7,10),
                        n.trees = c(500, 1000, 500), #He reducido la cantidad de arboles para reducir el tiempo de procesamiento y debido a que si mantenía los parametros que pretendía usar (500, 1000, 2000), me aparecía un error informando de que la memoría de mi ordenador era insuficiente.
                        shrinkage = c(.005, .02,.05),
                        n.minobsinnode = 10)



#En segundo lugar XGBoost.
tuneGridXGB <- expand.grid(
  nrounds=c(350),
  max_depth = c(2, 4, 6, 8),
  eta = c(0.05, 0.1),
  gamma = c(0.01),
  colsample_bytree = c(0.75),
  subsample = c(0.50),
  min_child_weight = c(0))

#Por ultimo definimos el de Random Forest.
#mtry max:
ncol(apps.df)-1
rfGrid <-  expand.grid(mtry = c(1,2,3,4,6,8))
```

Empezamos la comparación con el modelo boosting.

```{r results="hide"}
set.seed(2019)
### Gradient boosting machine algorithm. ###
fit.gbm <- train(Installs~. -App, data=dataset, method = 'gbm', trControl=fitControl, tuneGrid=gbmGrid, metric='Accuracy', distribution='multinomial')
```
```{r}

plot(fit.gbm)
#Como se puede ver en el gráfico el mejor "tune" corresponde a un shrinkage de 0.005, 500 arboles e "interaction depth"" de 10
fit.gbm$bestTune

res_gbm <- fit.gbm$results
acc_gbm <- subset(res_gbm[5])
# CV con mejor "tune"
BCV.acc = max(acc_gbm)

boost.caret.pred <- predict(fit.gbm,validation)
table(boost.caret.pred ,validation$Installs)
B.acc = mean(boost.caret.pred==validation$Installs)
```

En segundo lugar XGBoosting.

```{r}
set.seed(2019)
### EXtreme Gradient boosting algorithm. ###
fit.gbmx <- train(Installs~. -App, data=dataset, method = 'xgbTree', trControl=fitControl, tuneGrid=tuneGridXGB, metric='Accuracy', distribution='multinomial')

plot(fit.gbmx)
#Se muestran abajo los parámetros del mejor "tune" que se puede apreciar en la gráfica.
fit.gbmx$bestTune

res_gbmx <- fit.gbmx$results
acc_gbmx <- subset(res_gbmx[8])
# CV con mejor "tune"
XBCV.acc = max(acc_gbmx)


xboost.caret.pred <- predict(fit.gbmx,validation)
table(xboost.caret.pred ,validation$Installs)
XB.acc = mean(xboost.caret.pred==validation$Installs)

```

Por último Random Forest.

```{r}
set.seed(2019)
### Random Forest algorithm. ###
fit.rf <- train(Installs~. -App, data=dataset, method = 'rf', trControl=fitControl, tuneGrid=rfGrid, metric='Accuracy', distribution='multinomial')

plot(fit.rf)
#Como se puede ver en la gráfica el número óptimo de predictores seleccionados es 8.

res_rf <- fit.rf$results
acc_rf <- subset(res_rf[2]) 

# CV con mejor "tune" 
RFCV.acc = max(acc_rf)

rf.caret.pred <- predict(fit.rf,validation)
table(rf.caret.pred ,validation$Installs)
RF.acc = mean(rf.caret.pred==validation$Installs)
```

```{r tabla_resumen, include=FALSE}

Tabla_Resultados <- matrix( c(round(B.acc, 4), round(XB.acc, 4), round(RF.acc, 4)),
             nrow = 1, ncol = 3)
dimnames(Tabla_Resultados) <- list(c("Accuracy"), c("Boosting", "XGB", "Random Forest"))

which.min(Tabla_Resultados)
```
```{r echo=FALSE, fig.align="center", out.width='40%', out.height='40%', table-Resultados1}
knitr::kable(Tabla_Resultados, caption = "Resultados Accuracy")
```

Como se puede ver en la tabla, aunque todos los modelos están muy ajustados, el mejor resultado corresponde a eXtreme Gradient Boosting, acertando a la hora de clasificar con una probabilidad de `r round(XB.acc, 4)`.
Por lo tanto, será el modelo que se usará para la predicción evaluando en test usando el 100% de los datos.

```{r}
set.seed(2019)
fit.xgb_total <- train(Installs~. -App, data=total, method = 'xgbTree', trControl=fitControl, tuneGrid=tuneGridXGB, metric='Accuracy', distribution='multinomial')
plot(fit.xgb_total)
fit.xgb_total$bestTune

res_xgb_total <- fit.xgb_total$results
acc_xgb_total <- subset(res_xgb_total[8])
# CV con mejor "tune"
XBCV_T.acc = min(acc_xgb_total)

#Evaluamos en test
xboost.caret.pred_total <- predict(fit.xgb_total,test)
table(xboost.caret.pred_total ,test$Installs)
XB_T.acc = mean(xboost.caret.pred_total==test$Installs)
```

Como se puede ver en la tabla el modelo es muy acertado a la hora de clasificar la cantidad de descargas de una app, aún cuando falla suele fallar en el grupo adyacente y raramente en los más alejados.

```{r tabla_resumen2, include=FALSE}

Tabla_Resultados2 <- matrix( round(XB_T.acc, 4),
             nrow = 1, ncol = 1)
dimnames(Tabla_Resultados2) <- list(c("Accuracy del modelo"), c("eXtreme Gradient Boosting"))

which.min(Tabla_Resultados2)

```
```{r echo=FALSE, fig.align="center", out.width='40%', out.height='40%', table-Resultados2}
knitr::kable(Tabla_Resultados2, caption = "Resultados accuracy")
```

Se puede apreciar como usando todos los datos la probabilidad de acertar ha aumentado ligeramente a `r round(XB_T.acc, 4)`. 
Este resultado significa que este modelo acierta el `r round(XB_T.acc*100, 2)`% de las veces a la hora de clasificar la cantidad de descargas de una app. Lo cual aparentemente es una clasificación bastante precisa.